{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Sarsa](#Sarsa)\n",
    "* [Q-Learning](#Q-Learning)\n",
    "* [Expected Sarsa](#Expected-Sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As every action from each state is chosen using an epsilon-greedy\n",
    "# policy, there is no need to store an explicit policy. Instead this\n",
    "# function can be called whenever an acion needs to be selected.\n",
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    q_values = Q[state]\n",
    "    max_q = np.max(q_values)\n",
    "    rand = np.random.rand()\n",
    "    if rand <= epsilon:\n",
    "        return np.random.randint(len(q_values)) \n",
    "    else:\n",
    "        # Arbitarily chooses an action from tied (if any) max action-values\n",
    "        return np.random.choice(np.argwhere(q_values == max_q).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple method for evaluating how well a set of learnt action-values are.\n",
    "def success_rate(env, Q, runs):\n",
    "    goals_reached = 0\n",
    "    for episode in range(runs):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                goals_reached += reward\n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "    print(\"{} goals reached in {} runs.\".format(goals_reached, runs))\n",
    "    print(\"Accurracy: {}%\".format((goals_reached / runs) * 100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_sample_run(env, Q):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(Q[state])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print('Died!') if reward == 0 else print('Goal reached!')\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_sarsa(env, Q, episodes, gamma, epsilon, alpha):\n",
    "    for episode in range(episodes):\n",
    "        S = env.reset()\n",
    "        A = epsilon_greedy_policy(Q, S, epsilon)\n",
    "        while True:\n",
    "            next_S, R, done, _ = env.step(A)\n",
    "            next_A = epsilon_greedy_policy(Q, next_S, epsilon)\n",
    "            Q[S, A] = Q[S, A] + alpha * (R + gamma * Q[next_S, next_A] - Q[S, A])\n",
    "            S, A = next_S, next_A\n",
    "            if done:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table for all action-values\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "gamma = 1     # Discount factor \n",
    "epsilon = 0.1 # Chance of exploring by taking any action at random\n",
    "alpha = 0.1   # Update step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-values:\n",
      " [[0.38542883 0.36852483 0.34806161 0.36607753]\n",
      " [0.27419558 0.27512799 0.16057543 0.34169871]\n",
      " [0.29875965 0.27228659 0.26465511 0.30148754]\n",
      " [0.19177789 0.2312668  0.17135908 0.28112178]\n",
      " [0.41467919 0.35643179 0.37340066 0.22440828]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27862809 0.11815691 0.15116011 0.10331756]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.29388617 0.2837553  0.36303341 0.45771008]\n",
      " [0.40208719 0.59333051 0.36204683 0.24981677]\n",
      " [0.55679745 0.28499317 0.3739853  0.29937093]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47338288 0.44533824 0.72902758 0.45517896]\n",
      " [0.65252388 0.86656963 0.71640553 0.68136971]\n",
      " [0.         0.         0.         0.        ]]\n",
      "76.0 goals reached in 100 runs.\n",
      "Accurracy: 76.0%\n"
     ]
    }
   ],
   "source": [
    "on_policy_sarsa(env, Q, 100000, gamma, epsilon, alpha)\n",
    "print('Action-values:\\n', Q)\n",
    "success_rate(env, Q, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Goal reached!\n"
     ]
    }
   ],
   "source": [
    "view_sample_run(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, Q, episodes, gamma, epsilon, alpha):\n",
    "    for episode in range(episodes):\n",
    "        S = env.reset()\n",
    "        while True:\n",
    "            A = epsilon_greedy_policy(Q, S, epsilon)\n",
    "            next_S, R, done, _ = env.step(A)\n",
    "            Q[S, A] = Q[S, A] + alpha * (R + gamma * np.max(Q[next_S]) - Q[S, A])\n",
    "            S = next_S\n",
    "            if done:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table for all action-values\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "gamma = 0.9   # Discount factor \n",
    "epsilon = 0.1 # Chance of exploring by taking any action at random\n",
    "alpha = 0.1   # Update step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-values:\n",
      " [[0.0679356  0.04979244 0.0523198  0.04820485]\n",
      " [0.03128135 0.03942026 0.04478199 0.05523301]\n",
      " [0.07324997 0.05702781 0.05465407 0.04952464]\n",
      " [0.03138686 0.03322866 0.03090326 0.04789186]\n",
      " [0.09591423 0.05301047 0.06803787 0.04313018]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0662242  0.06791006 0.14534756 0.01748439]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07013989 0.06129739 0.08529915 0.13853839]\n",
      " [0.16067499 0.22097644 0.17818664 0.13653166]\n",
      " [0.25837143 0.18039743 0.20386642 0.09300748]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16466083 0.18498521 0.27491643 0.1801843 ]\n",
      " [0.3958154  0.40780153 0.59630805 0.39188552]\n",
      " [0.         0.         0.         0.        ]]\n",
      "75.0 goals reached in 100 runs.\n",
      "Accurracy: 75.0%\n"
     ]
    }
   ],
   "source": [
    "q_learning(env, Q, 100000, gamma, epsilon, alpha)\n",
    "print('Action-values:\\n', Q)\n",
    "success_rate(env, Q, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Goal reached!\n"
     ]
    }
   ],
   "source": [
    "view_sample_run(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env, Q, episodes, gamma, epsilon, alpha):\n",
    "    for episode in range(episodes):\n",
    "        S = env.reset()\n",
    "        while True:\n",
    "            A = epsilon_greedy_policy(Q, S, epsilon)\n",
    "            next_S, R, done, _ = env.step(A)\n",
    "            next_S_value = get_expected_value(Q, next_S, epsilon)\n",
    "            Q[S, A] = Q[S, A] + alpha * (R + gamma * next_S_value - Q[S, A])\n",
    "            S = next_S\n",
    "            if done:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the expeced value of a state by taking the weighted\n",
    "# average of all action-values based on the likliehood of them\n",
    "# being chosen from the epsilon-greedy policy.\n",
    "def get_expected_value(Q, state, epsilon):\n",
    "    q_values = Q[state]\n",
    "    best_action = np.argmax(q_values)\n",
    "    m = len(q_values)\n",
    "    state_value = 0\n",
    "    for action, q_value in enumerate(q_values):\n",
    "        if action == best_action:\n",
    "            # The max action has a (1-epislon) chance of being selected as the \n",
    "            # greedy action or an epsilon/m chance of being selected at random.\n",
    "            state_value += (1-epsilon + epsilon/m) * q_value\n",
    "        else:\n",
    "            state_value += epsilon/m * q_value\n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table for all action-values\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "gamma = 1     # Discount factor \n",
    "epsilon = 0.1 # Chance of exploring by taking any action at random\n",
    "alpha = 0.1   # Update step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-values:\n",
      " [[0.43460378 0.39341619 0.3980886  0.39239535]\n",
      " [0.24380414 0.30816516 0.20811536 0.37790806]\n",
      " [0.29584985 0.30589546 0.30180186 0.33861333]\n",
      " [0.24450326 0.22762405 0.21849362 0.31186174]\n",
      " [0.45591821 0.38801159 0.23483287 0.30467676]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35148348 0.16048533 0.18582421 0.13815091]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2824222  0.34477006 0.35369865 0.51021174]\n",
      " [0.35390799 0.58628965 0.45059996 0.36611898]\n",
      " [0.6117993  0.27794206 0.23756439 0.3131454 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37723615 0.51404779 0.71696412 0.52320994]\n",
      " [0.67611977 0.81479454 0.72735669 0.72165591]\n",
      " [0.         0.         0.         0.        ]]\n",
      "77.0 goals reached in 100 runs.\n",
      "Accurracy: 77.0%\n"
     ]
    }
   ],
   "source": [
    "expected_sarsa(env, Q, 10000, gamma, epsilon, alpha)\n",
    "print('Action-values:\\n', Q)\n",
    "success_rate(env, Q, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Goal reached!\n"
     ]
    }
   ],
   "source": [
    "view_sample_run(env, Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
