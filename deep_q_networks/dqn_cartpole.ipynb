{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "Build DQN that learns to play cartpole.\n",
    "\n",
    "* I'll need to build a NN that can backpropagate to update weights.we wouldn't have a target for the other action. I think I will have to go for the more inefficient architecture. \n",
    "    * Architecture:\n",
    "        * Inputs: 4 features from CartPole observation + which action is taken\n",
    "        * Outputs: the value for that state-action pair\n",
    "        * Will start with one hidden layer with ReLU and like the DQN paper the output will strictly be a linear layer\n",
    "        * Loss function: $\\frac{1}{2}(y - Q(s, a; w))^2$\n",
    "        * Target: $y = r + \\gamma \\max_{a'}Q(s', a'; w)$\n",
    "        * Will need to store training examples of $(s, a, r, s')$ in a database that holds the $N$ most recent examples which are chosen at random each iteration to be applied to gradient descent -- should try and apply mini-batch gradient descent\n",
    "        * Agent will move through environment collecting generating experience data using an $\\varepsilon$-greedy policy\n",
    "        * Will have to freeze the network to be used in updating weights for some set iteration amount (the frozen network is for the target, or oracle substitution, only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [] # Datatset for Experience Replay\n",
    "N = 200 # Number of most recent experiences stored in D\n",
    "model = {} # Contains weights in the NN\n",
    "h_n = 20 # Number of units in hidden layer\n",
    "input_size = env.action_space.n + env.observation_space.shape[0]\n",
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.9 # discount rate\n",
    "epsilon = 0.1 # exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialising weights of NN with Xavier initialisation \n",
    "model['W1'] = np.random.randn(h_n, input_size) / np.sqrt(input_size)\n",
    "model['b1'] = np.zeros((h_n, 1))\n",
    "model['W2'] = np.random.randn(1, h_n) / np.sqrt(h_n)\n",
    "model['b2'] = np.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_forward_pass(x):\n",
    "    h = np.dot(model['W1'], x) + model['b1']\n",
    "    h[h<0] = 0 # ReLU activation\n",
    "    q = np.dot(model['W2'], h) + model['b2']\n",
    "    return q, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates input array ready for NN\n",
    "def format_input(S, A):\n",
    "    format_A = [1, 0] if A == 0 else [0, 1]\n",
    "    return np.append(S, format_A).reshape(input_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation using a single example first\n",
    "def q_backward_pass(example):\n",
    "    S, A, R, SS = example\n",
    "    _, max_q = get_max_q(SS)\n",
    "    y = R + gamma * max_q\n",
    "    x = format_input(S, A)\n",
    "    q, h = q_forward_pass(x)\n",
    "    \n",
    "    # L = 1/2(y - q)^2\n",
    "    dq = -(y-q)\n",
    "    dW2 = np.dot(dq, h.T)\n",
    "    db2 = dq\n",
    "    dh = np.dot(model['W2'].T, dq)\n",
    "    dh[h<=0] = 0\n",
    "    dW1 = np.dot(dh, x.T)\n",
    "    db1 = dh\n",
    "    \n",
    "    # either update here or return derivatives so I can then average them over a minibatch?\n",
    "    # actually can't do that because then i'm not making use of vectorisation\n",
    "    # the question is how to store all examples in one big array. For now I will do stochastic\n",
    "    # gradient descent and use only one example to update weights and call it many times.\n",
    "    model['W1'] = model['W1'] - alpha * dW1\n",
    "    model['b1'] = model['b1'] - alpha * db1\n",
    "    model['W2'] = model['W2'] - alpha * dW2\n",
    "    model['b2'] = model['b2'] - alpha * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_q(state):\n",
    "    a1 = q_forward_pass(format_input(state, 0))\n",
    "    a2 = q_forward_pass(format_input(state, 1))\n",
    "    return np.argmax([a1, a2]), np.max([a1, a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1):\n",
    "    S = env.reset()\n",
    "    while True:\n",
    "        choice = np.random.rand()\n",
    "        if choice < epsilon:\n",
    "            A = env.action_space.sample()\n",
    "        else: \n",
    "            A, _ = get_max_q(S)\n",
    "        SS, R, done, _ = env.step(A)\n",
    "        example = (S, A, R, SS)\n",
    "        D = np.append(example, D[0:N-1]) # Adding new example to database and removing last if full\n",
    "        \n",
    "        # TODO perform steps of gradient descent\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
