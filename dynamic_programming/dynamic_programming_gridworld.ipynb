{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gridworld_utils import print_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "Finding the state-values for some given policy in a 4x4 gridworld; like the example in chapter 4.1 of Sutton and Barto's 'Rienforcement Learning: An Introduction' where \n",
    "* The top left and top right are terminal states \n",
    "* There are 4 actions; up, down, left, right. (If you move into the edge you stay where you are.)\n",
    "* You get a reward of -1 for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_value(policy, state_values, reward, discount):\n",
    "    v = 0\n",
    "    # Bellman Expectation Equation for deterministic environment. (Taking\n",
    "    # some action in a state will always lead to the same successor state).\n",
    "    for p, s_v in zip(policy, state_values):\n",
    "        v += p * (reward + discount * s_v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sweep_policy_evaluation(grid, policy):\n",
    "    n = grid.shape[0]\n",
    "    new_grid = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (i == 0 and j == 0) or (i == n-1 and j == n-1):\n",
    "                continue\n",
    "                \n",
    "            # Finding the values for each possible successor state\n",
    "            # when moving up or down.\n",
    "            if i == 0:\n",
    "                v_up = grid[i][j]\n",
    "                v_down = grid[i+1][j]\n",
    "            elif i == (n-1):\n",
    "                v_up = grid[i-1][j]\n",
    "                v_down = grid[i][j]\n",
    "            else:\n",
    "                v_up = grid[i-1][j]\n",
    "                v_down = grid[i+1][j]\n",
    "            # Finding the values for each possible successor state\n",
    "            # when moving left or right.\n",
    "            if j == 0:\n",
    "                v_left = grid[i][j]\n",
    "                v_right = grid[i][j+1]\n",
    "            elif j == (n-1):\n",
    "                v_left = grid[i][j-1]\n",
    "                v_right = grid[i][j]\n",
    "            else:\n",
    "                v_left = grid[i][j-1]\n",
    "                v_right = grid[i][j+1]\n",
    "            \n",
    "            new_grid[i][j] = update_state_value(policy, (v_up,v_down,v_left,v_right), -1, 1)\n",
    "    return new_grid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.zeros((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|======|======|======|======|\n",
      "|//////|-1.0  |-1.0  |-1.0  |\n",
      "|======|======|======|======|\n",
      "|-1.0  |-1.0  |-1.0  |-1.0  |\n",
      "|======|======|======|======|\n",
      "|-1.0  |-1.0  |-1.0  |-1.0  |\n",
      "|======|======|======|======|\n",
      "|-1.0  |-1.0  |-1.0  |//////|\n",
      "|======|======|======|======|\n"
     ]
    }
   ],
   "source": [
    "policy = (1/4, 1/4, 1/4, 1/4) # Random Policy\n",
    "grid = one_sweep_policy_evaluation(grid, policy)\n",
    "print_grid(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value states after iteration 1:\n",
      "|======|======|======|======|\n",
      "|//////|-1.0  |-1.0  |-1.0  |\n",
      "|======|======|======|======|\n",
      "|-1.0  |-1.0  |-1.0  |-1.0  |\n",
      "|======|======|======|======|\n",
      "|-1.0  |-1.0  |-1.0  |-1.0  |\n",
      "|======|======|======|======|\n",
      "|-1.0  |-1.0  |-1.0  |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 2:\n",
      "|======|======|======|======|\n",
      "|//////|-1.8  |-2.0  |-2.0  |\n",
      "|======|======|======|======|\n",
      "|-1.8  |-2.0  |-2.0  |-2.0  |\n",
      "|======|======|======|======|\n",
      "|-2.0  |-2.0  |-2.0  |-1.8  |\n",
      "|======|======|======|======|\n",
      "|-2.0  |-2.0  |-1.8  |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 3:\n",
      "|======|======|======|======|\n",
      "|//////|-2.4  |-2.9  |-3.0  |\n",
      "|======|======|======|======|\n",
      "|-2.4  |-2.9  |-3.0  |-2.9  |\n",
      "|======|======|======|======|\n",
      "|-2.9  |-3.0  |-2.9  |-2.4  |\n",
      "|======|======|======|======|\n",
      "|-3.0  |-2.9  |-2.4  |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 10:\n",
      "|======|======|======|======|\n",
      "|//////|-6.1  |-8.4  |-9.0  |\n",
      "|======|======|======|======|\n",
      "|-6.1  |-7.7  |-8.4  |-8.4  |\n",
      "|======|======|======|======|\n",
      "|-8.4  |-8.4  |-7.7  |-6.1  |\n",
      "|======|======|======|======|\n",
      "|-9.0  |-8.4  |-6.1  |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 20:\n",
      "|======|======|======|======|\n",
      "|//////|-9.4  |-13.3 |-14.5 |\n",
      "|======|======|======|======|\n",
      "|-9.4  |-12.1 |-13.3 |-13.3 |\n",
      "|======|======|======|======|\n",
      "|-13.3 |-13.3 |-12.1 |-9.4  |\n",
      "|======|======|======|======|\n",
      "|-14.5 |-13.3 |-9.4  |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 30:\n",
      "|======|======|======|======|\n",
      "|//////|-11.4 |-16.1 |-17.6 |\n",
      "|======|======|======|======|\n",
      "|-11.4 |-14.6 |-16.1 |-16.1 |\n",
      "|======|======|======|======|\n",
      "|-16.1 |-16.1 |-14.6 |-11.4 |\n",
      "|======|======|======|======|\n",
      "|-17.6 |-16.1 |-11.4 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 40:\n",
      "|======|======|======|======|\n",
      "|//////|-12.5 |-17.7 |-19.5 |\n",
      "|======|======|======|======|\n",
      "|-12.5 |-16.0 |-17.8 |-17.7 |\n",
      "|======|======|======|======|\n",
      "|-17.7 |-17.8 |-16.0 |-12.5 |\n",
      "|======|======|======|======|\n",
      "|-19.5 |-17.7 |-12.5 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 50:\n",
      "|======|======|======|======|\n",
      "|//////|-13.1 |-18.7 |-20.5 |\n",
      "|======|======|======|======|\n",
      "|-13.1 |-16.8 |-18.7 |-18.7 |\n",
      "|======|======|======|======|\n",
      "|-18.7 |-18.7 |-16.8 |-13.1 |\n",
      "|======|======|======|======|\n",
      "|-20.5 |-18.7 |-13.1 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 60:\n",
      "|======|======|======|======|\n",
      "|//////|-13.5 |-19.2 |-21.2 |\n",
      "|======|======|======|======|\n",
      "|-13.5 |-17.3 |-19.2 |-19.2 |\n",
      "|======|======|======|======|\n",
      "|-19.2 |-19.2 |-17.3 |-13.5 |\n",
      "|======|======|======|======|\n",
      "|-21.2 |-19.2 |-13.5 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 70:\n",
      "|======|======|======|======|\n",
      "|//////|-13.7 |-19.6 |-21.5 |\n",
      "|======|======|======|======|\n",
      "|-13.7 |-17.6 |-19.6 |-19.6 |\n",
      "|======|======|======|======|\n",
      "|-19.6 |-19.6 |-17.6 |-13.7 |\n",
      "|======|======|======|======|\n",
      "|-21.5 |-19.6 |-13.7 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 80:\n",
      "|======|======|======|======|\n",
      "|//////|-13.8 |-19.7 |-21.7 |\n",
      "|======|======|======|======|\n",
      "|-13.8 |-17.8 |-19.7 |-19.7 |\n",
      "|======|======|======|======|\n",
      "|-19.7 |-19.7 |-17.8 |-13.8 |\n",
      "|======|======|======|======|\n",
      "|-21.7 |-19.7 |-13.8 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 90:\n",
      "|======|======|======|======|\n",
      "|//////|-13.9 |-19.9 |-21.8 |\n",
      "|======|======|======|======|\n",
      "|-13.9 |-17.9 |-19.9 |-19.9 |\n",
      "|======|======|======|======|\n",
      "|-19.9 |-19.9 |-17.9 |-13.9 |\n",
      "|======|======|======|======|\n",
      "|-21.8 |-19.9 |-13.9 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 100:\n",
      "|======|======|======|======|\n",
      "|//////|-13.9 |-19.9 |-21.9 |\n",
      "|======|======|======|======|\n",
      "|-13.9 |-17.9 |-19.9 |-19.9 |\n",
      "|======|======|======|======|\n",
      "|-19.9 |-19.9 |-17.9 |-13.9 |\n",
      "|======|======|======|======|\n",
      "|-21.9 |-19.9 |-13.9 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 110:\n",
      "|======|======|======|======|\n",
      "|//////|-14.0 |-20.0 |-21.9 |\n",
      "|======|======|======|======|\n",
      "|-14.0 |-18.0 |-20.0 |-20.0 |\n",
      "|======|======|======|======|\n",
      "|-20.0 |-20.0 |-18.0 |-14.0 |\n",
      "|======|======|======|======|\n",
      "|-21.9 |-20.0 |-14.0 |//////|\n",
      "|======|======|======|======|\n",
      "\n",
      "Value states after iteration 120:\n",
      "|======|======|======|======|\n",
      "|//////|-14.0 |-20.0 |-22.0 |\n",
      "|======|======|======|======|\n",
      "|-14.0 |-18.0 |-20.0 |-20.0 |\n",
      "|======|======|======|======|\n",
      "|-20.0 |-20.0 |-18.0 |-14.0 |\n",
      "|======|======|======|======|\n",
      "|-22.0 |-20.0 |-14.0 |//////|\n",
      "|======|======|======|======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid = np.zeros((4,4))\n",
    "for k in range(1, 121):\n",
    "    grid = one_sweep_policy_evaluation(grid, policy)\n",
    "    if k <= 3 or k % 10 == 0:\n",
    "        print('Value states after iteration {}:'.format(k))\n",
    "        print_grid(grid)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
