{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gridworld_utils import print_grid_values, print_grid_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "Finding the state-values for a given policy in some gridworld where \n",
    "* There are 4 actions; left, up, right, down. (If you move into the edge you stay where you are.)\n",
    "* You get a reward of -1 for each step\n",
    "* Terminal states have a value of 0\n",
    "\n",
    "The first example is a 4x4 gridworld with two corner terminal states, like the example in chapter 4.1 of Sutton and Barto's 'Rienforcement Learning: An Introduction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_value(policy, state_values, reward=-1, discount=1):\n",
    "    v = 0\n",
    "    # Bellman Expectation Equation for deterministic environment. (Taking\n",
    "    # some action in some state will always lead to the same successor state).\n",
    "    for p, s_v in zip(policy, state_values):\n",
    "        v += p * (reward + discount * s_v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sweep_policy_evaluation(grid, policy, terminal_states):\n",
    "    rows, columns = grid.shape\n",
    "    new_grid = np.zeros(grid.shape)\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            \n",
    "            # Making sure not to update terminal states\n",
    "            is_terminal_state = False\n",
    "            for terminal_state in terminal_states:\n",
    "                x, y = terminal_state\n",
    "                if i == x and j == y:\n",
    "                    is_terminal_state = True\n",
    "            if is_terminal_state:\n",
    "                continue\n",
    "                \n",
    "            # Finding previous state values for all possible succussor states \n",
    "            v_left = grid[i][j] if j == 0 else grid[i][j-1]\n",
    "            v_up = grid[i][j] if i == 0 else grid[i-1][j]\n",
    "            v_right = grid[i][j] if j == (columns-1) else grid[i][j+1]\n",
    "            v_down = grid[i][j] if i == (rows-1) else grid[i+1][j]\n",
    "            \n",
    "            new_grid[i][j] = update_state_value(policy[i][j], (v_left,v_up,v_right,v_down))\n",
    "    return new_grid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_policy_evaluation(grid, policy, terminal_states, theta=0.01):\n",
    "    difference = 100 # Arbitrary large initial difference \n",
    "    iteration = 0 \n",
    "    \n",
    "    while difference > theta:\n",
    "        previous_grid = np.copy(grid)\n",
    "        grid = one_sweep_policy_evaluation(grid, policy, terminal_states)\n",
    "        difference = np.max(np.abs(previous_grid - grid))\n",
    "        iteration += 1\n",
    "    return grid, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_random_policy(shape, terminal_states):\n",
    "    policy = np.empty(shape, dtype=object)\n",
    "    policy.fill((1/4, 1/4, 1/4, 1/4))\n",
    "    for terminal_state in terminal_states:\n",
    "        policy[terminal_state] = None\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-values at iteration 0:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values at iteration 1:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -1.0  | -1.0  | -1.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.0  | -1.0  | -1.0  | -1.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.0  | -1.0  | -1.0  | -1.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.0  | -1.0  | -1.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values at iteration 2:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -1.8  | -2.0  | -2.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.8  | -2.0  | -2.0  | -2.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -2.0  | -1.8  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -1.8  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values at iteration 3:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -2.4  | -2.9  | -3.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.4  | -2.9  | -3.0  | -2.9  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.9  | -3.0  | -2.9  | -2.4  |\n",
      "|=======|=======|=======|=======|\n",
      "| -3.0  | -2.9  | -2.4  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_4x4 = np.zeros((4,4))\n",
    "terminal_states = ((0,0), (3,3)) # Top left and bottom right corners\n",
    "policy = initialise_random_policy(gridworld_4x4.shape, terminal_states)\n",
    "\n",
    "for k in range(4):\n",
    "    print('State-values at iteration {}:'.format(k))\n",
    "    print_grid_values(gridworld_4x4)\n",
    "    gridworld_4x4 = one_sweep_policy_evaluation(gridworld_4x4, policy, terminal_states)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-values after iteration 159:\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -27.0 | -25.0 | -22.5 | -22.5 | -25.0 | -27.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -25.0 | -21.5 | -16.0 | -16.0 | -21.5 | -25.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -22.5 | -16.0 |  0.0  |  0.0  | -16.0 | -22.5 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -22.5 | -16.0 |  0.0  |  0.0  | -16.0 | -22.5 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -25.0 | -21.5 | -16.0 | -16.0 | -21.5 | -25.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -27.0 | -25.0 | -22.5 | -22.5 | -25.0 | -27.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_6x6 = np.zeros((6,6))\n",
    "terminal_states = ((2,2), (2,3), (3,2), (3,3)) # The middle 4 squares\n",
    "policy = initialise_random_policy(gridworld_6x6.shape, terminal_states)\n",
    "theta = 0.001\n",
    "\n",
    "gridworld_6x6, k = full_policy_evaluation(gridworld_6x6, policy, terminal_states, theta)\n",
    "print('State-values after iteration {}:'.format(k))\n",
    "print_grid_values(gridworld_6x6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same gridworld rules as stated above.\n",
    "\n",
    "Whats happeing:\n",
    "1. Beginning with a random policy and all state-values set to 0\n",
    "2. Evaluating that policy all the way untill changes being made are less than set parameter theta (**Policy Evaluation**)\n",
    "3. Acting greedily on the values for that policy to generate a better policy (**Policy Improvement**)\n",
    "4. Repeating steps 2 and 3 untill the improved policies are equal to each other and thus equal to the optimal policy (**Policy Iteration**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an improved Policy by taking greedy actions with respect to the\n",
    "# immediate reward + value of next state\n",
    "def get_improved_policy(grid, terminal_states, reward=-1, discount=1):\n",
    "    rows, columns = grid.shape\n",
    "    policy = np.empty(grid.shape, dtype=object)\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            \n",
    "            # Making sure not to update terminal states\n",
    "            is_terminal_state = False\n",
    "            for terminal_state in terminal_states:\n",
    "                x, y = terminal_state\n",
    "                if i == x and j == y:\n",
    "                    is_terminal_state = True\n",
    "            if is_terminal_state:\n",
    "                continue\n",
    "                \n",
    "            # Finding previous state values for all possible succussor states \n",
    "            v_left = grid[i][j] if j == 0 else grid[i][j-1]\n",
    "            v_up = grid[i][j] if i == 0 else grid[i-1][j]\n",
    "            v_right = grid[i][j] if j == (columns-1) else grid[i][j+1]\n",
    "            v_down = grid[i][j] if i == (rows-1) else grid[i+1][j]\n",
    "            \n",
    "            # Finding the the values of every action from current state and\n",
    "            # acting greedly \n",
    "            successor_state_values = np.array([v_left, v_up, v_right, v_down])\n",
    "            action_values = reward + discount * successor_state_values\n",
    "            greedy_actions = action_values == np.max(action_values)\n",
    "            policy[i][j] = tuple(greedy_actions / np.count_nonzero(greedy_actions))\n",
    "    return policy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(grid, policy, terminal_states, theta):\n",
    "    print('Initial state-values and policy:')\n",
    "    print_grid_values(grid)\n",
    "    print_grid_policy(policy)\n",
    "\n",
    "    improved_policies = 0\n",
    "    total_k = 0\n",
    "    while True:\n",
    "        grid, k = full_policy_evaluation(grid, policy, terminal_states, theta)\n",
    "        total_k += k\n",
    "        \n",
    "        previous_policy = np.copy(policy)\n",
    "        policy = get_improved_policy(grid, terminal_states)\n",
    "        improved_policies += 1\n",
    "        \n",
    "        print('State-values and improved policy #{} (k={}):'.format(improved_policies, total_k))\n",
    "        print_grid_values(grid)\n",
    "        print_grid_policy(policy)\n",
    "        \n",
    "        if (previous_policy == policy).all():\n",
    "            print('Optimal policy found!')\n",
    "            return grid, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state-values and policy:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|\n",
      "|///////| ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  |///////|\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values and greedy policy w.r.t. v after 3 iterations:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -3.1  | -3.8  | -4.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -3.1  | -3.7  | -3.9  | -3.8  |\n",
      "|=======|=======|=======|=======|\n",
      "| -3.8  | -3.9  | -3.7  | -3.1  |\n",
      "|=======|=======|=======|=======|\n",
      "| -4.0  | -3.8  | -3.1  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |  ←↓   |\n",
      "|=======|=======|=======|=======|\n",
      "|   ↑   |  ←↑   |  ←↓   |   ↓   |\n",
      "|=======|=======|=======|=======|\n",
      "|   ↑   |  ↑→   |  →↓   |   ↓   |\n",
      "|=======|=======|=======|=======|\n",
      "|  ↑→   |   →   |   →   |///////|\n",
      "|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_4x4 = np.zeros((4,4))\n",
    "terminal_states = ((0,0), (3,3)) # Top left and bottom right corners\n",
    "policy = initialise_random_policy(gridworld_4x4.shape, terminal_states)\n",
    "\n",
    "print('Initial state-values and policy:')\n",
    "print_grid_values(gridworld_4x4)\n",
    "print_grid_policy(policy)\n",
    "\n",
    "for k in range(4):\n",
    "    gridworld_4x4 = one_sweep_policy_evaluation(gridworld_4x4, policy, terminal_states)\n",
    "print('State-values and greedy policy w.r.t. v after 3 iterations:')\n",
    "print_grid_values(gridworld_4x4)\n",
    "improved_policy = get_improved_policy(gridworld_4x4, terminal_states)\n",
    "print_grid_policy(improved_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state-values and policy:\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  |///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  |///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  |///////|///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |///////| ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "State-values and improved policy #1 (k=176):\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -29.7 | -29.1 | -29.5 | -33.9 | -39.6 | -44.9 | -48.7 | -50.6 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -26.3 | -24.1 | -21.5 | -28.5 | -36.2 | -42.3 | -46.5 | -48.6 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -21.2 | -15.4 |  0.0  | -18.6 | -30.2 | -37.8 | -42.5 | -44.8 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -18.0 | -12.3 |  0.0  | -11.7 | -24.1 | -32.3 | -36.9 | -39.2 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -16.4 | -11.8 |  0.0  |  0.0  | -18.4 | -26.3 | -29.7 | -32.0 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -15.4 | -14.5 | -12.1 | -13.3 | -19.0 | -21.0 | -19.5 | -23.2 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -11.3 | -14.6 | -16.7 | -18.3 | -19.2 | -15.4 |  0.0  | -14.1 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  | -12.1 | -17.6 | -20.0 | -20.1 | -17.2 | -12.1 | -15.1 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ↓   |   ↓   |   ↓   |   ←   |   ←   |   ←   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ↓   |   ↓   |   ↓   |   ←   |   ←   |   ←   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|   ←   |   ←   |   ←   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|  ←↓   |   ←   |   ←   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|///////|   ←   |   ←   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ↑   |   ↑   |   ↑   |   ←   |   ↓   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ←   |   ↑   |   ↑   |   →   |   →   |///////|   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |   ←   |   →   |   →   |   ↑   |   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "State-values and improved policy #2 (k=184):\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -4.0  | -3.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  | -7.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -3.0  | -2.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -4.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  |  0.0  | -1.0  | -2.0  | -2.0  | -3.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -1.0  | -1.0  | -2.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -1.0  | -2.0  | -2.0  | -2.0  | -2.0  | -1.0  |  0.0  | -1.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|   ←   |   ←   |   ←   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|  ←↓   |  ←↓   |  ←↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|///////|   ←   |   ←   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ↑→   |   ↑   |   ↑   |  ←↑   |  →↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ←↓   |   ↑   |   ↑   |   →   |   →   |///////|   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |  ←↑   |  ↑→   |  ↑→   |   ↑   |  ←↑   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "State-values and improved policy #3 (k=185):\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -4.0  | -3.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  | -7.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -3.0  | -2.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -4.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  |  0.0  | -1.0  | -2.0  | -2.0  | -3.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -1.0  | -1.0  | -2.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -1.0  | -2.0  | -2.0  | -2.0  | -2.0  | -1.0  |  0.0  | -1.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|   ←   |   ←   |   ←   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|  ←↓   |  ←↓   |  ←↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|///////|   ←   |   ←   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ↑→   |   ↑   |   ↑   |  ←↑   |  →↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ←↓   |   ↑   |   ↑   |   →   |   →   |///////|   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |  ←↑   |  ↑→   |  ↑→   |   ↑   |  ←↑   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "Optimal policy found!\n"
     ]
    }
   ],
   "source": [
    "gridworld_8x8 = np.zeros((8,8))\n",
    "terminal_states = ((2, 2), (3, 2), (4, 2), (4, 3), (6, 6), (7, 0))\n",
    "policy = initialise_random_policy(gridworld_8x8.shape, terminal_states)\n",
    "theta = 0.01\n",
    "\n",
    "v, pi = policy_iteration(gridworld_8x8, policy, terminal_states, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the hyperparameters above to create your own gridworld and then find the optimal value function and policy** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
