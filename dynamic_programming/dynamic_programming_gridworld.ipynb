{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Policy Evaluation](#Policy-Evaluation)\n",
    "* [Policy Iteration](#Policy-Iteration)\n",
    "* [Value Iteration](#Value-Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gridworld_utils import print_grid_values, print_grid_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridworld\n",
    "For all gridworlds in the following examples:\n",
    "* There are 4 actions - left, up, right, down\n",
    "* The environment is deterministic - taking some action from some state will always lead to the same successor state\n",
    "* Moving into the edge will cause you to stay where you are\n",
    "* Each move/step results in a reward of $-1$\n",
    "* Terminal states have a constant value of $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "Evaluating the exact state-values for a given policy in some gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_value(policy, state_values, reward=-1, discount=1):\n",
    "    v = 0\n",
    "    # Bellman Expectation Equation for deterministic environment. (Taking\n",
    "    # some action in some state will always lead to the same successor state).\n",
    "    for p, s_v in zip(policy, state_values):\n",
    "        v += p * (reward + discount * s_v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sweep_policy_evaluation(grid, policy, terminal_states):\n",
    "    rows, columns = grid.shape\n",
    "    new_grid = np.zeros(grid.shape)\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            \n",
    "            # Making sure not to update terminal states\n",
    "            is_terminal_state = False\n",
    "            for terminal_state in terminal_states:\n",
    "                x, y = terminal_state\n",
    "                if i == x and j == y:\n",
    "                    is_terminal_state = True\n",
    "            if is_terminal_state:\n",
    "                continue\n",
    "                \n",
    "            # Finding previous state values for all possible succussor states \n",
    "            v_left = grid[i][j] if j == 0 else grid[i][j-1]\n",
    "            v_up = grid[i][j] if i == 0 else grid[i-1][j]\n",
    "            v_right = grid[i][j] if j == (columns-1) else grid[i][j+1]\n",
    "            v_down = grid[i][j] if i == (rows-1) else grid[i+1][j]\n",
    "            \n",
    "            new_grid[i][j] = update_state_value(policy[i][j], (v_left,v_up,v_right,v_down))\n",
    "    return new_grid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_policy_evaluation(grid, policy, terminal_states, theta=0.01):\n",
    "    difference = 100 # Arbitrary large initial difference \n",
    "    iteration = 0 \n",
    "    \n",
    "    while difference > theta: # theta controls how accurately we continue to evaluate for\n",
    "        previous_grid = np.copy(grid)\n",
    "        grid = one_sweep_policy_evaluation(grid, policy, terminal_states)\n",
    "        difference = np.max(np.abs(previous_grid - grid))\n",
    "        iteration += 1\n",
    "    return grid, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_random_policy(shape, terminal_states):\n",
    "    policy = np.empty(shape, dtype=object)\n",
    "    policy.fill((1/4, 1/4, 1/4, 1/4))\n",
    "    for terminal_state in terminal_states:\n",
    "        policy[terminal_state] = None\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "This first example is a 4x4 gridworld with two corner terminal states, like the example in chapter 4.1 of Sutton and Barto's 'Rienforcement Learning: An Introduction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-values at iteration 0:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values at iteration 1:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -1.0  | -1.0  | -1.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.0  | -1.0  | -1.0  | -1.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.0  | -1.0  | -1.0  | -1.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.0  | -1.0  | -1.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values at iteration 2:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -1.8  | -2.0  | -2.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -1.8  | -2.0  | -2.0  | -2.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -2.0  | -1.8  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -1.8  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values at iteration 3:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -2.4  | -2.9  | -3.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.4  | -2.9  | -3.0  | -2.9  |\n",
      "|=======|=======|=======|=======|\n",
      "| -2.9  | -3.0  | -2.9  | -2.4  |\n",
      "|=======|=======|=======|=======|\n",
      "| -3.0  | -2.9  | -2.4  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_4x4 = np.zeros((4,4))\n",
    "terminal_states = ((0,0), (3,3)) # Top left and bottom right corners\n",
    "policy = initialise_random_policy(gridworld_4x4.shape, terminal_states)\n",
    "\n",
    "for k in range(4):\n",
    "    print('State-values at iteration {}:'.format(k))\n",
    "    print_grid_values(gridworld_4x4)\n",
    "    gridworld_4x4 = one_sweep_policy_evaluation(gridworld_4x4, policy, terminal_states)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-values after iteration 159:\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -27.0 | -25.0 | -22.5 | -22.5 | -25.0 | -27.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -25.0 | -21.5 | -16.0 | -16.0 | -21.5 | -25.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -22.5 | -16.0 |  0.0  |  0.0  | -16.0 | -22.5 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -22.5 | -16.0 |  0.0  |  0.0  | -16.0 | -22.5 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -25.0 | -21.5 | -16.0 | -16.0 | -21.5 | -25.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "| -27.0 | -25.0 | -22.5 | -22.5 | -25.0 | -27.0 |\n",
      "|=======|=======|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_6x6 = np.zeros((6,6))\n",
    "terminal_states = ((2,2), (2,3), (3,2), (3,3)) # The middle 4 squares\n",
    "policy = initialise_random_policy(gridworld_6x6.shape, terminal_states)\n",
    "theta = 0.001 \n",
    "\n",
    "gridworld_6x6, k = full_policy_evaluation(gridworld_6x6, policy, terminal_states, theta)\n",
    "print('State-values after iteration {}:'.format(k))\n",
    "print_grid_values(gridworld_6x6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats happeing:\n",
    "1. Beginning with a random policy and all state-values set to 0\n",
    "2. Evaluating that policy all the way untill changes being made are less than set parameter theta (**Policy Evaluation**)\n",
    "3. Acting greedily on the values for that policy to generate a better policy (**Policy Improvement**)\n",
    "4. Repeating steps 2 and 3 untill the improved policies are equal to each other and thus equal to the optimal policy (**Policy Iteration**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an improved Policy by taking greedy actions with respect to the\n",
    "# immediate reward + value of next state\n",
    "def get_greedy_policy(grid, terminal_states, reward=-1, discount=1):\n",
    "    rows, columns = grid.shape\n",
    "    policy = np.empty(grid.shape, dtype=object)\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            \n",
    "            # Making sure not to update terminal states\n",
    "            is_terminal_state = False\n",
    "            for terminal_state in terminal_states:\n",
    "                x, y = terminal_state\n",
    "                if i == x and j == y:\n",
    "                    is_terminal_state = True\n",
    "            if is_terminal_state:\n",
    "                continue\n",
    "                \n",
    "            # Finding previous state values for all possible succussor states \n",
    "            v_left = grid[i][j] if j == 0 else grid[i][j-1]\n",
    "            v_up = grid[i][j] if i == 0 else grid[i-1][j]\n",
    "            v_right = grid[i][j] if j == (columns-1) else grid[i][j+1]\n",
    "            v_down = grid[i][j] if i == (rows-1) else grid[i+1][j]\n",
    "            \n",
    "            # Finding the the values of every action from current state and\n",
    "            # acting greedly \n",
    "            successor_state_values = np.array([v_left, v_up, v_right, v_down])\n",
    "            action_values = reward + discount * successor_state_values\n",
    "            greedy_actions = action_values == np.max(action_values)\n",
    "            policy[i][j] = tuple(greedy_actions / np.count_nonzero(greedy_actions))\n",
    "    return policy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(grid, policy, terminal_states, theta):\n",
    "    print('Initial state-values and policy:')\n",
    "    print_grid_values(grid)\n",
    "    print_grid_policy(policy)\n",
    "\n",
    "    improved_policies = 0\n",
    "    total_k = 0\n",
    "    while True:\n",
    "        grid, k = full_policy_evaluation(grid, policy, terminal_states, theta)\n",
    "        total_k += k\n",
    "        \n",
    "        previous_policy = np.copy(policy)\n",
    "        policy = get_greedy_policy(grid, terminal_states)\n",
    "        improved_policies += 1\n",
    "        \n",
    "        if (previous_policy == policy).all():\n",
    "            print('Optimal policy found!')\n",
    "            return grid, policy\n",
    "        \n",
    "        print('State-values and improved policy #{} (k={}):'.format(improved_policies, total_k))\n",
    "        print_grid_values(grid)\n",
    "        print_grid_policy(policy)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state-values and policy:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|\n",
      "|///////| ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  |///////|\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "State-values and greedy policy w.r.t. v after 3 iterations:\n",
      "|=======|=======|=======|=======|\n",
      "|  0.0  | -3.1  | -3.8  | -4.0  |\n",
      "|=======|=======|=======|=======|\n",
      "| -3.1  | -3.7  | -3.9  | -3.8  |\n",
      "|=======|=======|=======|=======|\n",
      "| -3.8  | -3.9  | -3.7  | -3.1  |\n",
      "|=======|=======|=======|=======|\n",
      "| -4.0  | -3.8  | -3.1  |  0.0  |\n",
      "|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |  ←↓   |\n",
      "|=======|=======|=======|=======|\n",
      "|   ↑   |  ←↑   |  ←↓   |   ↓   |\n",
      "|=======|=======|=======|=======|\n",
      "|   ↑   |  ↑→   |  →↓   |   ↓   |\n",
      "|=======|=======|=======|=======|\n",
      "|  ↑→   |   →   |   →   |///////|\n",
      "|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_4x4 = np.zeros((4,4))\n",
    "terminal_states = ((0,0), (3,3)) # Top left and bottom right corners\n",
    "policy = initialise_random_policy(gridworld_4x4.shape, terminal_states)\n",
    "\n",
    "print('Initial state-values and policy:')\n",
    "print_grid_values(gridworld_4x4)\n",
    "print_grid_policy(policy)\n",
    "\n",
    "for k in range(4):\n",
    "    gridworld_4x4 = one_sweep_policy_evaluation(gridworld_4x4, policy, terminal_states)\n",
    "print('State-values and greedy policy w.r.t. v after 3 iterations:')\n",
    "print_grid_values(gridworld_4x4)\n",
    "improved_policy = get_greedy_policy(gridworld_4x4, terminal_states)\n",
    "print_grid_policy(improved_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state-values and policy:\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |  0.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  |///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  |///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  |///////|///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |///////| ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////| ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  | ←↑→↓  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "State-values and improved policy #1 (k=176):\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -29.7 | -29.1 | -29.5 | -33.9 | -39.6 | -44.9 | -48.7 | -50.6 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -26.3 | -24.1 | -21.5 | -28.5 | -36.2 | -42.3 | -46.5 | -48.6 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -21.2 | -15.4 |  0.0  | -18.6 | -30.2 | -37.8 | -42.5 | -44.8 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -18.0 | -12.3 |  0.0  | -11.7 | -24.1 | -32.3 | -36.9 | -39.2 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -16.4 | -11.8 |  0.0  |  0.0  | -18.4 | -26.3 | -29.7 | -32.0 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -15.4 | -14.5 | -12.1 | -13.3 | -19.0 | -21.0 | -19.5 | -23.2 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -11.3 | -14.6 | -16.7 | -18.3 | -19.2 | -15.4 |  0.0  | -14.1 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  | -12.1 | -17.6 | -20.0 | -20.1 | -17.2 | -12.1 | -15.1 |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ↓   |   ↓   |   ↓   |   ←   |   ←   |   ←   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ↓   |   ↓   |   ↓   |   ←   |   ←   |   ←   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|   ←   |   ←   |   ←   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|  ←↓   |   ←   |   ←   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|///////|   ←   |   ←   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ↑   |   ↑   |   ↑   |   ←   |   ↓   |   ↓   |   ↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |   ←   |   ↑   |   ↑   |   →   |   →   |///////|   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |   ←   |   →   |   →   |   ↑   |   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "State-values and improved policy #2 (k=184):\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -4.0  | -3.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  | -7.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -3.0  | -2.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -4.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  |  0.0  | -1.0  | -2.0  | -2.0  | -3.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -1.0  | -1.0  | -2.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -1.0  | -2.0  | -2.0  | -2.0  | -2.0  | -1.0  |  0.0  | -1.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|   ←   |   ←   |   ←   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|  ←↓   |  ←↓   |  ←↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|///////|   ←   |   ←   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ↑→   |   ↑   |   ↑   |  ←↑   |  →↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ←↓   |   ↑   |   ↑   |   →   |   →   |///////|   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |  ←↑   |  ↑→   |  ↑→   |   ↑   |  ←↑   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "Optimal policy found!\n"
     ]
    }
   ],
   "source": [
    "gridworld_8x8 = np.zeros((8,8))\n",
    "terminal_states = ((2, 2), (3, 2), (4, 2), (4, 3), (6, 6), (7, 0))\n",
    "policy = initialise_random_policy(gridworld_8x8.shape, terminal_states)\n",
    "theta = 0.01\n",
    "\n",
    "v, pi = policy_iteration(gridworld_8x8, policy, terminal_states, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "Value Iteration is simply always updating with the values with respect to the action that leads to the highest immediate reward + value of, in this case, the previous iteration (synchronous update). This can be achieved by performing Policy Iteration with only one sweep of updates to the values before choosing the new greedy policy and repeating. Concisely, you are just iteratively updating each state-value with respect to the Bellman Optimality Equation:\n",
    "\n",
    "$$\\mathcal{v}_{k+1}(s) = \\max_a \\sum_{s', r} p(s', r \\mid s, a) [r + \\gamma \\mathcal{v}_k(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, terminal_states, theta=0.01, reward=-1, discount=1):\n",
    "    rows, columns = grid.shape\n",
    "    iteration = 0\n",
    "    \n",
    "    # Looping through value iterations until updates beting made\n",
    "    # are less than theta.\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        pre_grid = np.copy(grid)\n",
    "        # Looping through every single state and updating its value.\n",
    "        for i in range(rows):\n",
    "            for j in range(columns):\n",
    "                \n",
    "                # Making sure not to update terminal states\n",
    "                is_terminal_state = False\n",
    "                for terminal_state in terminal_states:\n",
    "                    x, y = terminal_state\n",
    "                    if i == x and j == y:\n",
    "                        is_terminal_state = True\n",
    "                if is_terminal_state:\n",
    "                    continue\n",
    "\n",
    "                # Finding previous state values for all possible succussor states \n",
    "                v_left = pre_grid[i][j] if j == 0 else pre_grid[i][j-1]\n",
    "                v_up = pre_grid[i][j] if i == 0 else pre_grid[i-1][j]\n",
    "                v_right = pre_grid[i][j] if j == (columns-1) else pre_grid[i][j+1]\n",
    "                v_down = pre_grid[i][j] if i == (rows-1) else pre_grid[i+1][j]\n",
    "\n",
    "                # Bellman Optimality Equation (in this gridworld the probability\n",
    "                # of being in a state, s, and taking action, a, and getting a \n",
    "                # reward, r, and ending up in state, s', is always 100%).\n",
    "                successor_state_values = np.array([v_left, v_up, v_right, v_down])\n",
    "                action_values = reward + discount * successor_state_values\n",
    "                grid[i][j] = np.max(action_values)\n",
    "        \n",
    "        difference = np.max(np.abs(pre_grid - grid))\n",
    "        if difference < theta:\n",
    "            policy = get_greedy_policy(grid, terminal_states)\n",
    "            print('≈ Optimal value function and policy found ({} iterations):'.format(iteration))\n",
    "            print_grid_values(grid)\n",
    "            print_grid_policy(policy)\n",
    "            return grid, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≈ Optimal value function and policy found (8 iterations):\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -4.0  | -3.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  | -7.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -3.0  | -2.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  | -6.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -4.0  | -5.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -4.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -1.0  |  0.0  |  0.0  | -1.0  | -2.0  | -2.0  | -3.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -2.0  | -2.0  | -1.0  | -1.0  | -2.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "| -1.0  | -2.0  | -2.0  | -2.0  | -2.0  | -1.0  |  0.0  | -1.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  0.0  | -1.0  | -2.0  | -3.0  | -3.0  | -2.0  | -1.0  | -2.0  |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|  →↓   |  →↓   |   ↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|   ←   |   ←   |   ←   |  ←↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|  ←↓   |  ←↓   |  ←↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   →   |   →   |///////|///////|   ←   |   ←   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ↑→   |   ↑   |   ↑   |  ←↑   |  →↓   |   ↓   |  ←↓   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|   ↓   |  ←↓   |   ↑   |   ↑   |   →   |   →   |///////|   ←   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "|///////|   ←   |   ←   |  ←↑   |  ↑→   |  ↑→   |   ↑   |  ←↑   |\n",
      "|=======|=======|=======|=======|=======|=======|=======|=======|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridworld_8x8 = np.zeros((8,8))\n",
    "terminal_states = ((2, 2), (3, 2), (4, 2), (4, 3), (6, 6), (7, 0))\n",
    "policy = initialise_random_policy(gridworld_8x8.shape, terminal_states)\n",
    "theta = 0.01\n",
    "\n",
    "v, pi = value_iteration(gridworld_8x8, terminal_states, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note -- with the same theta only 8 state-value updates were required in Value Iteration compared to 184 in Policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the hyperparameters above to create a unique gridworld and find the optimal value function and policy** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
