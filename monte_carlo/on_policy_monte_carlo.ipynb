{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As every action from each state is chosen using an epsilon-greedy\n",
    "# policy, there is no need to store an explicit policy. Instead this\n",
    "#Â function can be called whenever an acion needs to be selected.\n",
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    q_values = Q[state]\n",
    "    max_q = np.max(q_values)\n",
    "    rand = np.random.rand()\n",
    "    if rand <= epsilon:\n",
    "        return np.random.randint(len(q_values)) \n",
    "    else:\n",
    "        # Arbitarily chooses an action from tied (if any) max action-values\n",
    "        return np.random.choice(np.argwhere(q_values == max_q).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_monte_carlo(env, Q, N, episodes, epsilon, gamma, render=False):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        sample = []\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            sample = np.append(sample, (action, reward, state))\n",
    "            if done:\n",
    "                sample = sample[:-1] # Removing redundant last state\n",
    "                G = 0 # G = Return\n",
    "                # Looping through sample backwards to incrementally adjust return \n",
    "                # and avoid having to calculate the same parts multiple times for \n",
    "                # all state-action pairs.\n",
    "                for i in range(len(sample)-1, 0, -3):\n",
    "                    S, A, R = int(sample[i-2]), int(sample[i-1]), sample[i]\n",
    "                    G = R + gamma * G\n",
    "                    N[S, A] = N[S, A] + 1 # N = count each state-action pair has been visited\n",
    "                    Q[S, A] = Q[S, A] + (1 / N[S, A]) * (G - Q[S, A])\n",
    "                break\n",
    "    env.close()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate(env, Q, runs):\n",
    "    goals_reached = 0\n",
    "    for episode in range(runs):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state])\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                goals_reached += reward\n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "    print(\"{} goals reached in {} runs.\".format(goals_reached, runs))\n",
    "    print(\"Accurracy: {}%\".format((goals_reached / runs) * 100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(16), Discrete(4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All action-values will be stored in Q (16 states all with 4 possible actions)\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "# Count for how many times each state-action pair has been visited\n",
    "N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "epsilon = 0.1\n",
    "gamma = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action-values:\n",
      " [[0.00305897 0.00260874 0.00242546 0.00165169]\n",
      " [0.00517387 0.00516486 0.00514099 0.00443338]\n",
      " [0.00770452 0.00877772 0.00868061 0.00175448]\n",
      " [0.00318169 0.00326304 0.0014384  0.00378942]\n",
      " [0.00801093 0.00663437 0.00615736 0.00269845]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.03075269 0.03097753 0.03319792 0.00315831]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00728788 0.02189034 0.01751979 0.02379878]\n",
      " [0.04172358 0.07296763 0.06747161 0.03773244]\n",
      " [0.13495538 0.11610201 0.11163687 0.02562988]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05258514 0.14948462 0.16324656 0.12893918]\n",
      " [0.17618077 0.48133033 0.47019283 0.39796134]\n",
      " [0.         0.         0.         0.        ]]\n",
      "70.0 goals reached in 100 runs.\n",
      "Accurracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "Q = on_policy_monte_carlo(env, Q, N, 100000, epsilon, gamma)\n",
    "print(\"Action-values:\\n\", Q)\n",
    "success_rate(env, Q, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
